{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoJQeumMzqW3c24k1OZN0t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takemyt1me/ML_practice/blob/main/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Automatic derivative"
      ],
      "metadata": {
        "id": "vP8uyGkRa8Lz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shS-6f3Waf89",
        "outputId": "413e96d1-0048-467f-86f3-0befcdfb57bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<tf.Tensor: shape=(), dtype=float32, numpy=8.0>]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "w = tf.Variable(2.)\n",
        "\n",
        "def f(w):\n",
        "  y=w**2\n",
        "  z=2*y+5\n",
        "\n",
        "  return z\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = f(w)\n",
        "\n",
        "gradients = tape.gradient(z, [w])\n",
        "print(gradients)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression practice"
      ],
      "metadata": {
        "id": "3x0qYaS0cM9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = tf.Variable(4.0)\n",
        "b = tf.Variable(1.0)\n",
        "\n",
        "@tf.function\n",
        "def hypothesis(x):\n",
        "  return w*x + b\n",
        "\n",
        "@tf.function\n",
        "def mse_loss(y_pred, y):\n",
        "  return tf.reduce_mean(tf.square(y_pred - y))\n",
        "\n",
        "x = [1,2,3,4,5,6,7,8,9]\n",
        "y = [11,22,33,44,53,66,77,87,95]\n",
        "\n",
        "optimizer = tf.optimizers.SGD(0.01)\n",
        "\n",
        "for i in range(301):\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    y_pred = hypothesis(x)\n",
        "    cost = mse_loss(y_pred, y)\n",
        "\n",
        "  gradient = tape.gradient(cost, [w, b])\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradient, [w, b]))\n",
        "\n",
        "  if i%10 == 0:\n",
        "    print(\"epoch: {:3} | w의 값 : {:5.4f} | b의 값 : {:5.4} | cost : {:5.6f}\".format(i, w.numpy(), b.numpy(), cost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_dUN_gicQJs",
        "outputId": "2d8f78f3-e400-4cc2-836c-0d2af7841d84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:   0 | w의 값 : 8.2133 | b의 값 : 1.664 | cost : 1402.555542\n",
            "epoch:  10 | w의 값 : 10.4971 | b의 값 : 1.977 | cost : 1.351182\n",
            "epoch:  20 | w의 값 : 10.5047 | b의 값 :  1.93 | cost : 1.328165\n",
            "epoch:  30 | w의 값 : 10.5119 | b의 값 : 1.884 | cost : 1.306967\n",
            "epoch:  40 | w의 값 : 10.5188 | b의 값 : 1.841 | cost : 1.287436\n",
            "epoch:  50 | w의 값 : 10.5254 | b의 값 : 1.799 | cost : 1.269459\n",
            "epoch:  60 | w의 값 : 10.5318 | b의 값 : 1.759 | cost : 1.252898\n",
            "epoch:  70 | w의 값 : 10.5379 | b의 값 : 1.721 | cost : 1.237644\n",
            "epoch:  80 | w의 값 : 10.5438 | b의 값 : 1.684 | cost : 1.223598\n",
            "epoch:  90 | w의 값 : 10.5494 | b의 값 : 1.648 | cost : 1.210658\n",
            "epoch: 100 | w의 값 : 10.5548 | b의 값 : 1.614 | cost : 1.198740\n",
            "epoch: 110 | w의 값 : 10.5600 | b의 값 : 1.582 | cost : 1.187767\n",
            "epoch: 120 | w의 값 : 10.5650 | b의 값 :  1.55 | cost : 1.177665\n",
            "epoch: 130 | w의 값 : 10.5697 | b의 값 :  1.52 | cost : 1.168354\n",
            "epoch: 140 | w의 값 : 10.5743 | b의 값 : 1.492 | cost : 1.159782\n",
            "epoch: 150 | w의 값 : 10.5787 | b의 값 : 1.464 | cost : 1.151890\n",
            "epoch: 160 | w의 값 : 10.5829 | b의 값 : 1.437 | cost : 1.144619\n",
            "epoch: 170 | w의 값 : 10.5870 | b의 값 : 1.412 | cost : 1.137924\n",
            "epoch: 180 | w의 값 : 10.5909 | b의 값 : 1.387 | cost : 1.131752\n",
            "epoch: 190 | w의 값 : 10.5946 | b의 값 : 1.364 | cost : 1.126073\n",
            "epoch: 200 | w의 값 : 10.5982 | b의 값 : 1.341 | cost : 1.120843\n",
            "epoch: 210 | w의 값 : 10.6016 | b의 값 :  1.32 | cost : 1.116026\n",
            "epoch: 220 | w의 값 : 10.6049 | b의 값 : 1.299 | cost : 1.111589\n",
            "epoch: 230 | w의 값 : 10.6081 | b의 값 : 1.279 | cost : 1.107504\n",
            "epoch: 240 | w의 값 : 10.6111 | b의 값 :  1.26 | cost : 1.103736\n",
            "epoch: 250 | w의 값 : 10.6140 | b의 값 : 1.242 | cost : 1.100273\n",
            "epoch: 260 | w의 값 : 10.6168 | b의 값 : 1.224 | cost : 1.097082\n",
            "epoch: 270 | w의 값 : 10.6195 | b의 값 : 1.207 | cost : 1.094143\n",
            "epoch: 280 | w의 값 : 10.6221 | b의 값 : 1.191 | cost : 1.091434\n",
            "epoch: 290 | w의 값 : 10.6245 | b의 값 : 1.176 | cost : 1.088940\n",
            "epoch: 300 | w의 값 : 10.6269 | b의 값 : 1.161 | cost : 1.086645\n"
          ]
        }
      ]
    }
  ]
}